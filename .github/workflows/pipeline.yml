name: End-to-End Data + Model Pipeline

on:
  schedule:
    - cron: "0 4 * * *"  # Daily 4 AM
  push:
    paths:
      - "src_versioning/**"
      - "dvc.yaml"
      - ".github/workflows/pipeline.yml"

permissions:
  contents: write

jobs:
  pipeline:
    runs-on: ubuntu-latest

    steps:
      # -------------------- 1Ô∏è‚É£ CHECKOUT CODE --------------------
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      # -------------------- 2Ô∏è‚É£ INSTALL PYTHON --------------------
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      # -------------------- 3Ô∏è‚É£ INSTALL REQUIREMENTS --------------------
      - name: Install dependencies
        run: |
          pip install -r requirements.txt

      # -------------------- 4Ô∏è‚É£ EXPORT ENV VARIABLES --------------------
      - name: Load environment variables
        run: echo "Environment ready"
        env:
          DB_USER: ${{ secrets.DB_USER }}
          DB_PASS: ${{ secrets.DB_PASS }}
          DB_HOST: ${{ secrets.DB_HOST }}
          DB_NAME: ${{ secrets.DB_NAME }}

      # -------------------- 5Ô∏è‚É£ TEST Postgres CONNECTION --------------------
      - name: Test PostgreSQL connection
        run: |
          python - <<'EOF'
          import os, psycopg2
          try:
              psycopg2.connect(
                host=os.getenv("DB_HOST"),
                user=os.getenv("DB_USER"),
                password=os.getenv("DB_PASS"),
                dbname=os.getenv("DB_NAME")
              )
              print("‚úÖ PostgreSQL connected")
          except Exception as e:
              print("‚ùå PostgreSQL connection failed:", e)
          EOF
        env:
          DB_USER: ${{ secrets.DB_USER }}
          DB_PASS: ${{ secrets.DB_PASS }}
          DB_HOST: ${{ secrets.DB_HOST }}
          DB_NAME: ${{ secrets.DB_NAME }}

      # -------------------- 6Ô∏è‚É£ CONFIGURE S3 REMOTE --------------------
      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v2
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ secrets.AWS_REGION }}


      # -------------------- 7Ô∏è‚É£ DOWNLOAD PREVIOUS DATA/MODEL --------------------
      - name: DVC Pull (fetch previous version)
        run: dvc pull || echo "No previous DVC data"

      # -------------------- 8Ô∏è‚É£ RUN DATA PREPROCESSING --------------------
      - name: Run data preprocessing
        env:
          DB_USER: ${{ secrets.DB_USER }}
          DB_PASS: ${{ secrets.DB_PASS }}
          DB_HOST: ${{ secrets.DB_HOST }}
          DB_NAME: ${{ secrets.DB_NAME }}
        run: |
          python src_versioning/data_preprocess.py --output "data/raw/gold_snapshot_latest.csv"
      
      # Add project root to PYTHONPATH for module imports
      - name: Add project root to PYTHONPATH
        run: |
          echo "PYTHONPATH=$PYTHONPATH:$(pwd)" >> $GITHUB_ENV

      # -------------------- 9Ô∏è‚É£ RUN PIPELINE (TRAIN + METADATA) --------------------
      - name: Run DVC Pipeline
        env:
          DB_USER: ${{ secrets.DB_USER }}
          DB_PASS: ${{ secrets.DB_PASS }}
          DB_HOST: ${{ secrets.DB_HOST }}
          DB_NAME: ${{ secrets.DB_NAME }}
        run:  |
         git config --global user.name "github-actions"
          git config --global user.email "github-actions@github.com"

          git pull origin main || true
          echo "üßπ Cleaning Git-tracked DVC outputs if needed..."
          if git ls-files --error-unmatch data/raw/gold_snapshot_latest.csv >/dev/null 2>&1; then
            git rm --cached data/raw/gold_snapshot_latest.csv
          fi

          if git ls-files --error-unmatch models/gold_lstm_model.pkl >/dev/null 2>&1; then
            git rm --cached models/gold_lstm_model.pkl
          fi

          git commit -m "üßπ Stop tracking DVC-managed artifacts" || echo "No DVC-tracked files were in Git"
          echo "üöÄ Running DVC pipeline..."
          dvc repro

      # -------------------- üîü COMMIT UPDATED ARTIFACTS --------------------
      - name: Commit changes (models + metadata)
        run: |
           git config --global user.name "github-actions"
           git config --global user.email "github-actions@github.com"

           # ensure rebase is used
           git config pull.rebase true

           # fetch latest remote branch
           git fetch origin main

           # rebase onto remote branch to avoid divergent errors
           git rebase origin/main || echo "No remote changes to rebase"

           # add tracked files only (no errors if files missing)
           git add dvc.yaml dvc.lock models/ data/raw/ || echo "No files to add"

           # commit if there are changes
           git diff --cached --quiet || git commit -m "üöÄ Automated retraining + metadata update"

           # push to remote (force-with-lease is safer than --force)
           git push origin main --force-with-lease

      # -------------------- 1Ô∏è‚É£1Ô∏è‚É£ PUSH TO S3 --------------------
      - name: Push new DVC artifacts to S3
        run: dvc push