name: Data + Model Pipeline

on:
  schedule:
    - cron: "0 4 * * *"  # runs every day at 4 AM (adjust if needed)
  push:
    paths:
      - "src_versioning/**"
      - "dvc.yaml"
      - ".github/workflows/pipeline.yml"

permissions:
  contents: write

jobs:
  run_pipeline:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install dependencies
        run: |
          pip install -r requirements.txt
      
       # 4Ô∏è‚É£ Set environment variables (Postgres credentials)
      - name: Set environment variables
        env:
          DB_USER: ${{ secrets.DB_USER }}
          DB_PASS: ${{ secrets.DB_PASS }}
          DB_HOST: ${{ secrets.DB_HOST }}
          DB_NAME: ${{ secrets.DB_NAME }}
        run: |
          echo "‚úÖ Environment variables loaded"

      # 5Ô∏è‚É£ Test PostgreSQL connection
      - name: Test PostgreSQL connection
        env:
          DB_USER: ${{ secrets.DB_USER }}
          DB_PASS: ${{ secrets.DB_PASS }}
          DB_HOST: ${{ secrets.DB_HOST }}
          DB_NAME: ${{ secrets.DB_NAME }}
        run: |
          python - <<'EOF'
          import os
          import psycopg2

          try:
              conn = psycopg2.connect(
                  host=os.getenv("DB_HOST"),
                  user=os.getenv("DB_USER"),
                  password=os.getenv("DB_PASS"),
                  dbname=os.getenv("DB_NAME")
              )
              print("‚úÖ PostgreSQL connection successful")
              conn.close()
          except Exception as e:
              print("‚ùå PostgreSQL connection failed:", e)
          EOF


      - name: Get current date
        id: date
        run: echo "date=$(date +'%Y-%m-%d')" >> $GITHUB_OUTPUT

      - name: Run data preprocessing and save snapshot
        env:
          DB_USER: ${{ secrets.DB_USER }}
          DB_PASS: ${{ secrets.DB_PASS }}
          DB_HOST: ${{ secrets.DB_HOST }}
          DB_NAME: ${{ secrets.DB_NAME }}
        run: |
            # the below command is to generate files with dates
            # python src_versioning/data_preprocess.py --output "data/raw/gold_snapshot_${{ steps.date.outputs.date }}.csv"

            python src_versioning/data_preprocess.py --output "data/raw/gold_snapshot_latest.csv"


      - name: Run DVC pipeline (data + model)
        env:
          DB_USER: ${{ secrets.DB_USER }}
          DB_PASS: ${{ secrets.DB_PASS }}
          DB_HOST: ${{ secrets.DB_HOST }}
          DB_NAME: ${{ secrets.DB_NAME }}
        run: dvc repro

      - name: Commit and push model
        run: |
          git config --global user.name "github-actions"
          git config --global user.email "github-actions@github.com"

          git fetch --prune --unshallow || echo "Already full clone"
          git checkout main
          git pull origin main

          # Track updated pipeline + lock + metadata
          git add dvc.yaml dvc.lock .gitignore models/ || echo "No tracked files to add"

          git commit -m "üèãÔ∏è‚Äç‚ôÇÔ∏è Retrained model and updated metadata on ${{ steps.date.outputs.date }}" || echo "No model changes"
          git push origin main

      # Optional: add remote storage later
      - name: Push data & model
        run: dvc push