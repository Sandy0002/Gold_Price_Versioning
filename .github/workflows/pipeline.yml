name: Data + Model Pipeline

on:
  # workflow_dispatch:
  # schedule:
  #   - cron: "45 14 * * *"   # Optional: Runs daily at 20:15 IST

  push:
    branches: [ main ]
  pull_request:
    branches: [ main ]

jobs:
  run_pipeline:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install dependencies
        run: |
          pip install -r requirements.txt
      
       # 4Ô∏è‚É£ Set environment variables (Postgres credentials)
      - name: Set environment variables
        env:
          DB_USER: ${{ secrets.DB_USER }}
          DB_PASS: ${{ secrets.DB_PASS }}
          DB_HOST: ${{ secrets.DB_HOST }}
          DB_NAME: ${{ secrets.DB_NAME }}
        run: |
          echo "‚úÖ Environment variables loaded"

      # 5Ô∏è‚É£ Test PostgreSQL connection
      - name: Test PostgreSQL connection
        env:
          DB_USER: ${{ secrets.DB_USER }}
          DB_PASS: ${{ secrets.DB_PASS }}
          DB_HOST: ${{ secrets.DB_HOST }}
          DB_NAME: ${{ secrets.DB_NAME }}
        run: |
          python - <<'EOF'
          import os
          import psycopg2

          try:
              conn = psycopg2.connect(
                  host=os.getenv("DB_HOST"),
                  user=os.getenv("DB_USER"),
                  password=os.getenv("DB_PASS"),
                  dbname=os.getenv("DB_NAME")
              )
              print("‚úÖ PostgreSQL connection successful")
              conn.close()
          except Exception as e:
              print("‚ùå PostgreSQL connection failed:", e)
          EOF


      - name: Get current date
        id: date
        run: echo "date=$(date +'%Y-%m-%d')" >> $GITHUB_OUTPUT

      - name: Run data preprocessing and save snapshot
        env:
          DB_USER: ${{ secrets.DB_USER }}
          DB_PASS: ${{ secrets.DB_PASS }}
          DB_HOST: ${{ secrets.DB_HOST }}
          DB_NAME: ${{ secrets.DB_NAME }}
        run: |
            # the below command is to generate files with dates
            # python src_versioning/data_preprocess.py --output "data/raw/gold_snapshot_${{ steps.date.outputs.date }}.csv"

            python src_versioning/data_preprocess.py --output "data/raw/gold_snapshot_latest.csv"

      - name: Track new snapshot with DVC
        run: |
          git config --global user.name "github-actions"
          git config --global user.email "github-actions@github.com"

          git fetch --prune --unshallow || echo "Already full clone"
          git checkout main
          git pull origin main

          # ‚úÖ Just commit tracked outputs and pipeline metadata
          git add dvc.yaml dvc.lock data/raw/.gitignore
          git commit -m "üì¶ Add latest data snapshot via DVC pipeline" || echo "No changes to commit"
          git push origin main

      - name: Run DVC pipeline (data + model)
        run: dvc repro

      - name: Commit and push model
        run: |
          git add models/*.dvc dvc.yaml dvc.lock .gitignore || echo "No model artifacts found"
          git commit -m "üèãÔ∏è‚Äç‚ôÇÔ∏è Retrained model on data ${{ steps.date.outputs.date }}" || echo "No model changes"
          git push origin main

      # Optional: add remote storage later
      # - name: Push DVC data to remote
      #   run: dvc push