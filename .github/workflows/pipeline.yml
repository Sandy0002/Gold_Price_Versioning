name: End-to-End Data + Model Pipeline

on:
  schedule:
    - cron: "0 11 * * *"  # Daily 4 AM
  push:
    paths:
      - "src_versioning/**"
      - "dvc.yaml"
      - ".github/workflows/pipeline.yml"
      - "requirements.txt"

permissions:
  contents: write

jobs:
  pipeline:
    runs-on: ubuntu-latest

    steps:
      # -------------------- 1Ô∏è‚É£ CHECKOUT CODE --------------------
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      # -------------------- 2Ô∏è‚É£ INSTALL PYTHON --------------------
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      # -------------------- 3Ô∏è‚É£ INSTALL REQUIREMENTS --------------------
      - name: Install dependencies
        run: |
          pip install -r requirements.txt

      # -------------------- 4Ô∏è‚É£ EXPORT ENV VARIABLES --------------------
      - name: Load environment variables
        run: echo "Environment ready"
        env:
          DB_USER: ${{ secrets.DB_USER }}
          DB_PASS: ${{ secrets.DB_PASS }}
          DB_HOST: ${{ secrets.DB_HOST }}
          DB_NAME: ${{ secrets.DB_NAME }}
          TABLE_NAME: ${{secrets.TABLE_NAME}}

      # -------------------- 5Ô∏è‚É£ TEST Postgres CONNECTION --------------------
      - name: Test PostgreSQL connection
        run: |
          python - <<'EOF'
          import os, psycopg2
          try:
              psycopg2.connect(
                host=os.getenv("DB_HOST"),
                user=os.getenv("DB_USER"),
                password=os.getenv("DB_PASS"),
                dbname=os.getenv("DB_NAME"),
                tablename=os.getenv("TABLE_NAME")
              )
              print("‚úÖ PostgreSQL connected")
          except Exception as e:
              print("‚ùå PostgreSQL connection failed:", e)
          EOF
        env:
          DB_USER: ${{ secrets.DB_USER }}
          DB_PASS: ${{ secrets.DB_PASS }}
          DB_HOST: ${{ secrets.DB_HOST }}
          DB_NAME: ${{ secrets.DB_NAME }}
          TABLE_NAME: ${{secrets.TABLE_NAME}}

      # -------------------- 6Ô∏è‚É£ CONFIGURE S3 REMOTE --------------------
      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v2
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ secrets.AWS_REGION }}

      # Add project root to PYTHONPATH for module imports
      - name: Add project root to PYTHONPATH
        run: |
          echo "PYTHONPATH=$PYTHONPATH:$(pwd)" >> $GITHUB_ENV

      # -------------------- 9Ô∏è‚É£ RUN PIPELINE (TRAIN + METADATA) --------------------
      - name: Run DVC Pipeline
        env:
          DB_USER: ${{ secrets.DB_USER }}
          DB_PASS: ${{ secrets.DB_PASS }}
          DB_HOST: ${{ secrets.DB_HOST }}
          DB_NAME: ${{ secrets.DB_NAME }}
          TABLE_NAME: ${{secrets.TABLE_NAME}}
          
        run: |
            # -------------------- CONFIGURE GIT --------------------
            git config --global user.name "github-actions"
            git config --global user.email "github-actions@github.com"

            # -------------------- SYNC WITH REMOTE --------------------
            git fetch origin main
            git rebase origin/main || echo "No remote changes"

            # -------------------- CLEAN GIT-TRACKED DVC OUTPUTS --------------------
            echo "üßπ Checking DVC outputs for Git tracking..."

            # Data file
            if git ls-files --error-unmatch data/raw/gold_snapshot_latest.csv >/dev/null 2>&1; then
              echo "‚ùå data/raw/gold_snapshot_latest.csv is tracked by Git, removing..."
              git rm --cached data/raw/gold_snapshot_latest.csv
              git commit -m "üßπ Stop tracking data/raw/gold_snapshot_latest.csv" || echo "No commit needed"
            fi

            # Model file
            if git ls-files --error-unmatch models/gold_lstm_model.pkl >/dev/null 2>&1; then
              echo "‚ùå models/gold_lstm_model.pkl is tracked by Git, removing..."
              git rm --cached models/gold_lstm_model.pkl
              git commit -m "üßπ Stop tracking models/gold_lstm_model.pkl" || echo "No commit needed"
            fi

            # Metadata file
            if git ls-files --error-unmatch models/model_metadata.json >/dev/null 2>&1; then
              echo "‚ùå models/model_metadata.json is tracked by Git, removing..."
              git rm --cached models/model_metadata.json
              git commit -m "üßπ Stop tracking models/model_metadata.json" || echo "No commit needed"
            fi

            # -------------------- RUN DVC PIPELINE --------------------
            echo "üöÄ Running DVC pipeline..."
            dvc repro


      # -------------------- üîü COMMIT UPDATED ARTIFACTS --------------------
      - name: Commit changes (models + metadata)
        run: |
           git config --global user.name "github-actions"
           git config --global user.email "github-actions@github.com"

           # Add .dvc files and outputs (metadata)
           git add dvc.yaml dvc.lock models/ data/raw/ || echo "No files to add"

           # commit only if changes exist
           git diff --cached --quiet || git commit -m "üöÄ Automated retraining + metadata update"

           git push origin main --force-with-lease

      # -------------------- 1Ô∏è‚É£1Ô∏è‚É£ PUSH TO S3 --------------------
      - name: Push new DVC artifacts to S3
        run: dvc push